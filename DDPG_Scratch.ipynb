{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG_Scratch.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sGbtFRo8SerRtd-EP2-Hks7BDpqoCEOf","authorship_tag":"ABX9TyO/oUzFZzmt9Y3UgasMN1sT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"egWmakGkba2V"},"source":["import numpy as np\n","import tensorflow as tf\n","import gym\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","from sys import exit\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VNooWA-wT3O"},"source":["# **Model Generator Function**\n","\n","In the original DDPG paper by David Silver, for the critic network, the action enters the network in middle layers instead of entering the network from the beginning. This is only done to increase performance/stability.\n","\n","However, for us to learn from scratch, the action and state input will enter the critic network from the beginning. We write a function that generates both the actor and critic.\n"]},{"cell_type":"code","metadata":{"id":"1efGpu9Ab14Q"},"source":["# simple NN Generator\n","\n","def ANN2(input_shape,layer_sizes, hidden_activation='relu', output_activation=None):\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Input(shape=input_shape))    \n","    for h in layer_sizes[:-1]:\n","        x = model.add(tf.keras.layers.Dense(units=h, activation='relu'))\n","    model.add(tf.keras.layers.Dense(units=layer_sizes[-1], activation=output_activation))\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9UZJQc8xhNP"},"source":["The function ANN2 generates both critic and actor networks using input_shape and layer_size parameters. The hidden layers for both networks have ‘relu’ activations. The output layer for the actor will be a ‘tanh’, ( to map continuous action -1 to 1) and the output layer for critic will be ‘None’ as its the Q-value. The output for the actor-network can be scaled by a factor to make the action correspond to the environment action range."]},{"cell_type":"markdown","metadata":{"id":"kONqicW2yj1I"},"source":["# **Model Initialization**\n","\n","We initialize 4-networks: The main Actor and Critic and the target Actor and Critic"]},{"cell_type":"code","metadata":{"id":"0nyKcYpZyhYq"},"source":["# Network parameters\n","X_shape = (num_states)\n","QA_shape = (num_states + num_actions)\n","hidden_sizes_1=(1000,500,200)\n","hidden_sizes_2=(400,200)\n","\n","# Main network outputs\n","mu = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n","q_mu = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')\n","\n","# Target networks\n","mu_target = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n","q_mu_target = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2H-4gm2fzEPc"},"source":["# **Replay Buffer**\n","\n","As with other deep reinforcement learning techniques, DDPG relies on the use of Replay Buffer for stability. The replay buffer needs to maintain a balance of old and new experiences.\n","\n","***Definition:***\n","The replay buffer contains a collection of experience tuples (S, A, R, S′). The tuples are gradually added to the buffer as we are interacting with the Environment. The simplest implementation is a buffer of fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it.\n","\n","***Purpose:***\n","A buffer of past experiences is used to stabilize training by decorrelating the training examples in each batch used to update the neural network. This buffer records past states, the actions taken at those states, the reward received and the next state that was observed."]},{"cell_type":"code","metadata":{"id":"9eMJmox-0wBJ"},"source":["class BasicBuffer:\n","    \n","    def __init__(self, size, obs_dim, act_dim):\n","        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n","        self.rews_buf = np.zeros([size], dtype=np.float32)\n","        self.done_buf = np.zeros([size], dtype=np.float32)\n","        self.ptr, self.size, self.max_size = 0, 0, size\n","\n","    def push(self, obs, act, rew, next_obs, done):\n","        self.obs1_buf[self.ptr] = obs\n","        self.obs2_buf[self.ptr] = next_obs\n","        self.acts_buf[self.ptr] = act\n","        self.rews_buf[self.ptr] = np.asarray([rew])\n","        self.done_buf[self.ptr] = done\n","        self.ptr = (self.ptr+1) % self.max_size\n","        self.size = min(self.size+1, self.max_size)\n","\n","    def sample(self, batch_size=32):\n","        idxs = np.random.randint(0, self.size, size=batch_size)\n","        temp_dict= dict(s=self.obs1_buf[idxs],\n","                    s2=self.obs2_buf[idxs],\n","                    a=self.acts_buf[idxs],\n","                    r=self.rews_buf[idxs],\n","                    d=self.done_buf[idxs])\n","        return (temp_dict['s'],temp_dict['a'],temp_dict['r'].reshape(-1,1),temp_dict['s2'],temp_dict['d'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"daqmOR_qxUYj"},"source":["# Buffer Import\n","\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Colab_Data/DDPG')\n","\n","import buffer\n","from buffer import BasicBuffer_a,BasicBuffer_b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGisF9NF051A"},"source":[""],"execution_count":null,"outputs":[]}]}