{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG_pendulum","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/rl/ipynb/ddpg_pendulum.ipynb","timestamp":1627401312877}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCeHXmL9MpUp","executionInfo":{"status":"ok","timestamp":1629725353865,"user_tz":-360,"elapsed":27157,"user":{"displayName":"M Mashrukh Zayed 20445034","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6JQ-VWXmmHIUvrj2doUt-vEGNy4yKkapIGrP_=s64","userId":"05654379288344026358"}},"outputId":"259360cc-e48f-4275-e640-236f1acb09f0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vAQ9Zy1xq3rX"},"source":["import gym\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5M17KJ2Fq3rZ"},"source":["I used [OpenAIGym](http://gym.openai.com/docs) to create the environment.\n","I will use the `upper_bound` parameter to scale our actions later."]},{"cell_type":"code","metadata":{"id":"uj8BfgqRq3ra","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629725361088,"user_tz":-360,"elapsed":11,"user":{"displayName":"M Mashrukh Zayed 20445034","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6JQ-VWXmmHIUvrj2doUt-vEGNy4yKkapIGrP_=s64","userId":"05654379288344026358"}},"outputId":"4650a15a-e76f-40b8-b8f3-710ca3612e99"},"source":["problem = \"Pendulum-v0\"\n","env = gym.make(problem)\n","\n","num_states = env.observation_space.shape[0]\n","print(\"Size of State Space ->  {}\".format(num_states))\n","num_actions = env.action_space.shape[0]\n","print(\"Size of Action Space ->  {}\".format(num_actions))\n","\n","upper_bound = env.action_space.high[0]\n","lower_bound = env.action_space.low[0]\n","\n","print(\"Max Value of Action ->  {}\".format(upper_bound))\n","print(\"Min Value of Action ->  {}\".format(lower_bound))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of State Space ->  3\n","Size of Action Space ->  1\n","Max Value of Action ->  2.0\n","Min Value of Action ->  -2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XNlYs-8Tq3ra"},"source":["To implement better exploration by the Actor network, we use noisy perturbations,\n","specifically\n","an **Ornstein-Uhlenbeck process** for generating noise, as described in the paper.\n","It samples noise from a correlated normal distribution."]},{"cell_type":"code","metadata":{"id":"qApUKZNcq3rb"},"source":["\n","class OUActionNoise:\n","    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n","        self.theta = theta\n","        self.mean = mean\n","        self.std_dev = std_deviation\n","        self.dt = dt\n","        self.x_initial = x_initial\n","        self.reset()\n","\n","    def __call__(self):\n","        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n","        x = (\n","            self.x_prev\n","            + self.theta * (self.mean - self.x_prev) * self.dt\n","            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n","        )\n","        # Store x into x_prev\n","        # Makes next noise dependent on current one\n","        self.x_prev = x\n","        return x\n","\n","    def reset(self):\n","        if self.x_initial is not None:\n","            self.x_prev = self.x_initial\n","        else:\n","            self.x_prev = np.zeros_like(self.mean)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wo4USWB4q3rg"},"source":["# Replay Buffer\n","\n","class Buffer:\n","    def __init__(self, buffer_capacity=100000, batch_size=64):\n","        # Number of \"experiences\" to store at max\n","        self.buffer_capacity = buffer_capacity\n","        # Num of tuples to train on.\n","        self.batch_size = batch_size\n","\n","        # Its tells us num of times record() was called.\n","        self.buffer_counter = 0\n","\n","        # Instead of list of tuples as the exp.replay concept go\n","        # We use different np.arrays for each tuple element\n","        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n","        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n","        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n","        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n","\n","\n","    # Takes (s,a,r,s') obervation tuple as input\n","    def record(self, obs_tuple):\n","        # Set index to zero if buffer_capacity is exceeded,\n","        # replacing old records\n","        index = self.buffer_counter % self.buffer_capacity\n","\n","        self.state_buffer[index] = obs_tuple[0]\n","        self.action_buffer[index] = obs_tuple[1]\n","        self.reward_buffer[index] = obs_tuple[2]\n","        self.next_state_buffer[index] = obs_tuple[3]\n","\n","        self.buffer_counter += 1\n","\n","    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n","    # TensorFlow to build a static graph out of the logic and computations in our function.\n","    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n","\n","\n","    @tf.function\n","    def update(\n","        self, state_batch, action_batch, reward_batch, next_state_batch,\n","    ):\n","        # Training and updating Actor & Critic networks.\n","        # See Pseudo Code.\n","        with tf.GradientTape() as tape:\n","            target_actions = target_actor(next_state_batch, training=True)\n","            y = reward_batch + gamma * target_critic(\n","                [next_state_batch, target_actions], training=True\n","            )\n","            critic_value = critic_model([state_batch, action_batch], training=True)\n","            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n","\n","        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n","        critic_optimizer.apply_gradients(\n","            zip(critic_grad, critic_model.trainable_variables)\n","        )\n","\n","        with tf.GradientTape() as tape:\n","            actions = actor_model(state_batch, training=True)\n","            critic_value = critic_model([state_batch, actions], training=True)\n","            # Used `-value` as we want to maximize the value given\n","            # by the critic for our actions\n","            actor_loss = -tf.math.reduce_mean(critic_value)\n","\n","        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n","        actor_optimizer.apply_gradients(\n","            zip(actor_grad, actor_model.trainable_variables)\n","        )\n","\n","\n","    # We compute the loss and update parameters\n","    def learn(self):\n","        # Get sampling range\n","        record_range = min(self.buffer_counter, self.buffer_capacity)\n","        # Randomly sample indices\n","        batch_indices = np.random.choice(record_range, self.batch_size)\n","\n","        # Convert to tensors\n","        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n","        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n","        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n","        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n","        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n","\n","        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n","\n","\n","# This update target parameters slowly\n","# Based on rate `tau`, which is much less than one.\n","\n","\n","@tf.function\n","def update_target(target_weights, weights, tau):\n","    for (a, b) in zip(target_weights, weights):\n","        a.assign(b * tau + a * (1 - tau))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9l1MO8Zgq3ri"},"source":["Here we define the Actor and Critic networks. These are basic Dense models\n","with `ReLU` activation.\n","\n","Note: We need the initialization for last layer of the Actor to be between\n","`-0.003` and `0.003` as this prevents us from getting `1` or `-1` output values in\n","the initial stages, which would squash our gradients to zero,\n","as we use the `tanh` activation."]},{"cell_type":"code","metadata":{"id":"nReb6UErq3rj"},"source":["\n","def get_actor():\n","    # Initialize weights between -3e-3 and 3-e3\n","    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n","\n","    inputs = layers.Input(shape=(num_states,))\n","    out = layers.Dense(256, activation=\"relu\")(inputs)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n","\n","    # Our upper bound is 2.0 for Pendulum.\n","    outputs = outputs * upper_bound\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","\n","def get_critic():\n","    # State as input\n","    state_input = layers.Input(shape=(num_states))\n","    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n","    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n","\n","    # Action as input\n","    action_input = layers.Input(shape=(num_actions))\n","    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n","\n","    # Both are passed through seperate layer before concatenating\n","    concat = layers.Concatenate()([state_out, action_out])\n","\n","    out = layers.Dense(256, activation=\"relu\")(concat)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1)(out)\n","\n","    # Outputs single value for give state-action\n","    model = tf.keras.Model([state_input, action_input], outputs)\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vhYM2wEq3rk"},"source":["`policy()` returns an action sampled from our Actor network plus some noise for\n","exploration."]},{"cell_type":"code","metadata":{"id":"jokB5pD8q3rk"},"source":["\n","def policy(state, noise_object):\n","    sampled_actions = tf.squeeze(actor_model(state))\n","    noise = noise_object()\n","    # Adding noise to action\n","    sampled_actions = sampled_actions.numpy() + noise\n","\n","    # We make sure action is within bounds\n","    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n","\n","    return [np.squeeze(legal_action)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTCrPNyBq3rl"},"source":["## Training hyperparameters"]},{"cell_type":"code","metadata":{"id":"AAvwtUn0q3rm"},"source":["std_dev = 0.2\n","ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n","\n","actor_model = get_actor()\n","critic_model = get_critic()\n","\n","target_actor = get_actor()\n","target_critic = get_critic()\n","\n","# Making the weights equal initially\n","target_actor.set_weights(actor_model.get_weights())\n","target_critic.set_weights(critic_model.get_weights())\n","\n","# Learning rate for actor-critic models\n","critic_lr = 0.002\n","actor_lr = 0.001\n","\n","critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n","actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n","\n","total_episodes = 100\n","# Discount factor for future rewards\n","gamma = 0.99\n","# Used to update target networks\n","tau = 0.005\n","\n","buffer = Buffer(50000, 64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQxS4Dg4q3rn"},"source":["Now we implement our main training loop, and iterate over episodes.\n","We sample actions using `policy()` and train with `learn()` at each time step,\n","along with updating the Target networks at a rate `tau`."]},{"cell_type":"code","metadata":{"id":"kYdAmNaOq3rn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1629692532670,"user_tz":-360,"elapsed":221978,"user":{"displayName":"M Mashrukh Zayed 20445034","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6JQ-VWXmmHIUvrj2doUt-vEGNy4yKkapIGrP_=s64","userId":"05654379288344026358"}},"outputId":"7b9ce997-27b9-4f5d-d35e-939222cfaa01"},"source":["# To store reward history of each episode\n","ep_reward_list = []\n","# To store average reward history of last few episodes\n","avg_reward_list = []\n","\n","# Takes about 4 min to train\n","for ep in range(total_episodes):\n","\n","    prev_state = env.reset()\n","    episodic_reward = 0\n","\n","    while True:\n","        # Uncomment this to see the Actor in action\n","        # But not in a python notebook.\n","        # env.render()\n","\n","        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n","\n","        action = policy(tf_prev_state, ou_noise)\n","        # Recieve state and reward from environment.\n","        state, reward, done, info = env.step(action)\n","\n","        buffer.record((prev_state, action, reward, state))\n","        episodic_reward += reward\n","\n","        buffer.learn()\n","        update_target(target_actor.variables, actor_model.variables, tau)\n","        update_target(target_critic.variables, critic_model.variables, tau)\n","\n","        # End this episode when `done` is True\n","        if done:\n","            break\n","\n","        prev_state = state\n","\n","    ep_reward_list.append(episodic_reward)\n","\n","    # Mean of last 40 episodes\n","    avg_reward = np.mean(ep_reward_list[-40:])\n","    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n","    avg_reward_list.append(avg_reward)\n","\n","# Plotting graph\n","# Episodes versus Avg. Rewards\n","plt.plot(avg_reward_list)\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Avg. Epsiodic Reward\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Episode * 0 * Avg Reward is ==> -1531.4069200985643\n","Episode * 1 * Avg Reward is ==> -1397.5714982450831\n","Episode * 2 * Avg Reward is ==> -1532.914839069317\n","Episode * 3 * Avg Reward is ==> -1481.6623021475446\n","Episode * 4 * Avg Reward is ==> -1482.881859421525\n","Episode * 5 * Avg Reward is ==> -1448.6344923011613\n","Episode * 6 * Avg Reward is ==> -1390.4279721017883\n","Episode * 7 * Avg Reward is ==> -1380.934378761328\n","Episode * 8 * Avg Reward is ==> -1346.9085583937494\n","Episode * 9 * Avg Reward is ==> -1291.8233432257057\n","Episode * 10 * Avg Reward is ==> -1257.6706394827008\n","Episode * 11 * Avg Reward is ==> -1229.7089508615438\n","Episode * 12 * Avg Reward is ==> -1195.909034870703\n","Episode * 13 * Avg Reward is ==> -1149.1897123325414\n","Episode * 14 * Avg Reward is ==> -1123.4515070127793\n","Episode * 15 * Avg Reward is ==> -1068.3670550284364\n","Episode * 16 * Avg Reward is ==> -1005.8060924376795\n","Episode * 17 * Avg Reward is ==> -964.5252631017429\n","Episode * 18 * Avg Reward is ==> -932.0277360696357\n","Episode * 19 * Avg Reward is ==> -891.69501884327\n","Episode * 20 * Avg Reward is ==> -860.6453865173706\n","Episode * 21 * Avg Reward is ==> -821.6027284391729\n","Episode * 22 * Avg Reward is ==> -796.0992401799957\n","Episode * 23 * Avg Reward is ==> -768.1814508327035\n","Episode * 24 * Avg Reward is ==> -742.2941529370597\n","Episode * 25 * Avg Reward is ==> -718.2042131063225\n","Episode * 26 * Avg Reward is ==> -696.3351569756992\n","Episode * 27 * Avg Reward is ==> -675.840961255044\n","Episode * 28 * Avg Reward is ==> -652.6319661008464\n","Episode * 29 * Avg Reward is ==> -640.6014894156775\n","Episode * 30 * Avg Reward is ==> -627.6343297139998\n","Episode * 31 * Avg Reward is ==> -618.7346819051681\n","Episode * 32 * Avg Reward is ==> -607.4152378262316\n","Episode * 33 * Avg Reward is ==> -593.1945184794852\n","Episode * 34 * Avg Reward is ==> -583.0740125863528\n","Episode * 35 * Avg Reward is ==> -573.8581181310956\n","Episode * 36 * Avg Reward is ==> -558.4104662732422\n","Episode * 37 * Avg Reward is ==> -547.0630542503905\n","Episode * 38 * Avg Reward is ==> -536.2238981183958\n","Episode * 39 * Avg Reward is ==> -525.9997206422084\n","Episode * 40 * Avg Reward is ==> -493.84231583289727\n","Episode * 41 * Avg Reward is ==> -465.38376753747144\n","Episode * 42 * Avg Reward is ==> -423.4664283572468\n","Episode * 43 * Avg Reward is ==> -393.4380146139052\n","Episode * 44 * Avg Reward is ==> -362.1878334478982\n","Episode * 45 * Avg Reward is ==> -333.41347278060096\n","Episode * 46 * Avg Reward is ==> -313.69673380355925\n","Episode * 47 * Avg Reward is ==> -284.0698342529269\n","Episode * 48 * Avg Reward is ==> -260.0817040745539\n","Episode * 49 * Avg Reward is ==> -240.1894213778804\n","Episode * 50 * Avg Reward is ==> -226.01379799178358\n","Episode * 51 * Avg Reward is ==> -212.774661204539\n","Episode * 52 * Avg Reward is ==> -195.91041334401294\n","Episode * 53 * Avg Reward is ==> -188.32946899137488\n","Episode * 54 * Avg Reward is ==> -172.24378316834432\n","Episode * 55 * Avg Reward is ==> -175.58127373556485\n","Episode * 56 * Avg Reward is ==> -175.49012205112484\n","Episode * 57 * Avg Reward is ==> -175.01350378941603\n","Episode * 58 * Avg Reward is ==> -169.31923099239557\n","Episode * 59 * Avg Reward is ==> -169.22388371685847\n","Episode * 60 * Avg Reward is ==> -169.17622782784005\n","Episode * 61 * Avg Reward is ==> -172.19965558290323\n","Episode * 62 * Avg Reward is ==> -169.47376196388183\n","Episode * 63 * Avg Reward is ==> -172.24023163253372\n","Episode * 64 * Avg Reward is ==> -172.31330269483755\n","Episode * 65 * Avg Reward is ==> -177.65367830865907\n","Episode * 66 * Avg Reward is ==> -182.88532205695466\n","Episode * 67 * Avg Reward is ==> -185.64053492151714\n","Episode * 68 * Avg Reward is ==> -188.47896294594767\n","Episode * 69 * Avg Reward is ==> -181.2044288844715\n","Episode * 70 * Avg Reward is ==> -178.42760618255298\n","Episode * 71 * Avg Reward is ==> -172.9481915213142\n","Episode * 72 * Avg Reward is ==> -173.06321294503846\n","Episode * 73 * Avg Reward is ==> -170.0046426507825\n","Episode * 74 * Avg Reward is ==> -169.7703525567863\n","Episode * 75 * Avg Reward is ==> -163.52071766836258\n","Episode * 76 * Avg Reward is ==> -171.81567152406768\n","Episode * 77 * Avg Reward is ==> -176.67951785503288\n","Episode * 78 * Avg Reward is ==> -179.3591852276631\n","Episode * 79 * Avg Reward is ==> -176.2113773720844\n","Episode * 80 * Avg Reward is ==> -173.28193181871387\n","Episode * 81 * Avg Reward is ==> -173.2076715024204\n","Episode * 82 * Avg Reward is ==> -170.08123525955324\n","Episode * 83 * Avg Reward is ==> -173.20444109995523\n","Episode * 84 * Avg Reward is ==> -170.5601233783446\n","Episode * 85 * Avg Reward is ==> -170.6299192821078\n","Episode * 86 * Avg Reward is ==> -167.4925376633272\n","Episode * 87 * Avg Reward is ==> -167.55349543178187\n","Episode * 88 * Avg Reward is ==> -167.652949720225\n","Episode * 89 * Avg Reward is ==> -173.5687276765862\n","Episode * 90 * Avg Reward is ==> -167.8873567119104\n","Episode * 91 * Avg Reward is ==> -161.0821168092375\n","Episode * 92 * Avg Reward is ==> -164.1173326336748\n","Episode * 93 * Avg Reward is ==> -161.09126674416697\n","Episode * 94 * Avg Reward is ==> -161.32438739863804\n","Episode * 95 * Avg Reward is ==> -155.0072207454808\n","Episode * 96 * Avg Reward is ==> -157.99287054849128\n","Episode * 97 * Avg Reward is ==> -154.87297703975065\n","Episode * 98 * Avg Reward is ==> -155.13583862254123\n","Episode * 99 * Avg Reward is ==> -157.88604593345605\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hc1bX38e+yJMsqbpJs3LspNgZjBJgOphOI6ZDAhSSEkkAgHQL3JuQS8ibkXlpC8sYvkEBIwKabajA1EAyWe7eFmyQ3WZaLJKvOev+YIxCOyljSaEaj3+d55tGcfc7RWcMxs7TL2dvcHRERkbboFusARESk81MyERGRNlMyERGRNlMyERGRNlMyERGRNkuOdQCxkpOT4yNGjIh1GCIincq8efO2u3u/fcu7bDIZMWIEeXl5sQ5DRKRTMbMNjZWrmUtERNpMyURERNpMyURERNpMyURERNpMyURERNpMyURERNpMyURERNqsyz5nIiLSme2qqOH9NcUU76liWFY6I7LTSeuexIaSCtZuL6d4TxXdDJLMSOuexKA+aQztm87QrDR6p6VgZu0aj5KJiHR61bUhVm7ZzZj+maR373xfa3UhJ2/9Dt5ZtQ3DGJmTzvDsDKpqQ2woKWf99grKq2rp1g26mbFmWxnzNpRSF2rdelSv3XIi4wb1atfP0Pn+q4tIo9w9or82a+pCLCzYybKiXdSGHHdwvvhS6mbG4D5pDM/OYHCfNErKqygs3cuWXZX0Tk9haN90hmSlkZ6ShJlhQLdu7ftXbmN2VdSwvqScop17qakLEXKnrLKWD/O38+Ga7ZRX15GZmsz5hw/i8qOGcviQ3q3+69vdKS6rYl1xOZW1IUZkpzO4TxrJSd0or6qlaOdetpdVgQMGVbUhNpZUsG57OL7wooPha5tBN4PkpG4cPqQ3Jx3Yj4MO6MmO8mr+9VkJH67ZzuwVWykpryYlyXCH2n2SRFpKEr3TUqhzx93p37MH3zl5NKce3J8R2ekUlO5lQ0k55VV1jMhOZ0ROBgN69cAJJ6q91XUU7qygYMdeCksrGJ6d3rab0Qjrqist5ubmuqZTkUSwaedefjhjIUsKdzGyXwaj+2VyQK8euIcTRZ07tXVObSjEll2VfLpuB+XVde0aw4jsdCaPymbyqGyOGZXFwN5p+/07KqprWbF5D3PWlvDxZyWs3LI72GNU19axu7K20fMG9e7BqQf3J3dEX/65ZjuvLdlMZU2I7IzuTBrelyOH9+XAAzIZ0jecEMqqalm3vZx128tZX1LOxpIKNpRUsLuyhvqvw117ayir+vL1UpKMjNRkdlbUNPkZMronMaRvOslBUnD4/D5U1NRSsGMvAH3SUz7/PT17JHPygf04a/wATjmoH2kpSWzeVcn6knJSk5MYkZ1Ov56p7d4s1VpmNs/dc/+tXMlEpPN6d+U2fjBjITW1IaYeMZjC0r18tq2MkvIqDAv+KjZSkoyUpG70Skth8qgsThiTw6ThfemRkoTB5zUMCNdcCkv3sqGkgk0795KV0Z2hWekM7N2D0orqz/+6raoNhZNVKMTyzXv4dF3J51/4Q7PSOGZkNuMH9Qra6cNt9fVNUO7O4sJdvLJ4E3PXl1JYWsH2surPP9fBA3py+JA+JCcZIQ9/kQ/tm87w7HSG9E0nNaUbSWZ0T+7GwN49vvRFu7uyhjeWbOGTdTuYv7GUddvLm/zv1z2pG0Oy0hielU7f9O5gYBg9eyQzMieDETkZ9EjuxoaSCtaVlLN7bw2D+6YxpG86/TJTMQMP4huWXV/W9Jf+5l17+efq7cxdv4Ph2ekcPyaHCYN7k5zUecZCdZpkYma/A84HqoHPgG+6+85g38+Aa4E64BZ3nxWUnw08CCQBj7j7b1q6jpKJdFahkDN/YynP5BUyPa+AQwb24o9XTmJkTkZM46oLOSu37OaTtTv4ZF0Jn67bQek+f8XnZKYyLCuN7WXVbNxRQUqSMWlYX0bmZDA0K51RORkcPTKL7MzUdotrR3k160vKKSzdS1HpXjJSkxiZk8HInAwG9k4jqQOa6BJJZ0omZwLvuHutmf0WwN1vM7NxwFPA0cAgYDZwYHDaauAMoBCYC3zN3Zc3dx0lE+ls6kLOI/9cyxMfb6Bo5156pHTjiqOGcfs5B9MjJSnW4f0bd6ekvJqCHRVs3FFBYeleNpaE3/dI6cY5EwZy1rgB9E5PiXWosh+aSiZx1wHv7m822JwDXBK8nwo87e5VwDozyyecWADy3X0tgJk9HRzbbDIR6Ux2VlTz/ekLeW9VMcePyeZHZx7ImeMHkJkad/8Lf87MyMlMJSczlSOG9Y11OBJl8fsvMexbwPTg/WDCyaVeYVAGULBP+TGN/TIzux64HmDYsGHtGqhItCzftJsbn5zH5l17+dUFh3LlMcPipjNWpF5MkomZzQYGNLLrTnd/KTjmTqAW+Ht7XdfdpwHTINzM1V6/VyQaKmvqePjdfP7v+5+RldGd6TccyyT9hS9xKibJxN1Pb26/mX0DOA84zb/o1CkChjY4bEhQRjPlIp3SB6uL+a+XlrKhpIILjxjMnV85hJx27JQWaW9x18wVjMz6KXCyu1c02DUT+IeZ3Ue4A34s8CnhJ4PGmtlIwknkCuDrHRu1SPvYVVHD3a8u59l5hYzKyeAf3z6G48bkxDoskRbFXTIB/gCkAm8F7cJz3P1Gd19mZjMId6zXAje5ex2Amd0MzCI8NPgxd18Wm9BFWu/tFVv52fNLKCmv5runjOaW08bG5SgtkcbE3dDgjqKhwRJPHvnnWn716goOHtCT311yOBOG9I51SCKN6jRDg0W6Enfnf99czR/ezefcCQO4//KJpCarNiKdj5KJSIyEQs7PZy7lyTkbueKoodxz4QQ9jS2dlpKJSIz85o2VPDlnIzecPIrbzz5Yz45Ip9Z5ZhcTSSCPfriOaR+s5ZpjhyuRSEJQMhHpYK8s3sSvXl3OOYcO4Ofnj1cikYSgZCLSgRYW7OSH0xdx1PAs7r98ovpIJGEomYh0kD2VNdzy1AL69Uxl2tVH6hkSSSjqgBfpID9/aRmFpRXMuOFY+qR3j3U4Iu1KNRORDvD8/EJeWFDELaeNJXdEVqzDEWl3SiYiUbZi827+68WlHDWiLzefOibW4YhEhZKJSBTlrd/B5X/+mMweyTxwxRGdaq1vkf2hf9kiUfLOyq1c9egn5GSm8tx3jmNwn7RYhyQSNeqAF4mC91Zt4/on5nHwwJ789ZtHay0SSXhKJiLtbPOuvfxg+kLGHtCTp66bTM8eKbEOSSTq1Mwl0o5q60Lc8tQCqmpDPPz1I5RIpMtQzUSkHT0wew1z15fywOUTGdUvM9bhiHQY1UxE2slH+dt5+L18Ls8dygVHDI51OCIdSslEpB3sqqjhRzMWMSong7u+Oj7W4Yh0OCUTkXZw18vLKC6r4v7LJ5LWXXNuSdcTt8nEzH5kZm5mOcG2mdlDZpZvZovNbFKDY68xszXB65rYRS1d0etLNvPCgiK+N2UMhw3pE+twRGIiLjvgzWwocCawsUHxOcDY4HUM8CfgGDPLAn4B5AIOzDOzme5e2rFRS1dUvKeKO15YwoTBvblJU6VIFxavNZP7gZ8STg71pgJPeNgcoI+ZDQTOAt5y9x1BAnkLOLvDI5Yux9257bnFlFfXcd9lh5OiqVKkC4u7f/1mNhUocvdF++waDBQ02C4Mypoqb+x3X29meWaWV1xc3I5RS1f0xMcbeGflNu489xDGHtAz1uGIxFRMmrnMbDYwoJFddwJ3EG7ianfuPg2YBpCbm+stHC7SpNVb93DPays49aB+XH3s8FiHIxJzMUkm7n56Y+VmNgEYCSwK1sUeAsw3s6OBImBog8OHBGVFwCn7lL/X7kGLBCpr6rjlqQX06pHM7y49XGu4ixBnzVzuvsTd+7v7CHcfQbjJapK7bwFmAlcHo7omA7vcfTMwCzjTzPqaWV/CtZpZsfoMkvgeensNK7fs4XeXHq4JHEUCcTmaqwmvAecC+UAF8E0Ad99hZncDc4Pj/tvdd8QmREl02/ZU8thH67hg4iBOPah/rMMRiRtxnUyC2kn9ewduauK4x4DHOigs6cL+9N5n1NQ53z/9wFiHIhJX4qqZSySebd61l79/spGLJw1mRE5GrMMRiStKJiIRevjdfEIh53tTxsY6FJG4o2QiEoHC0gqmzy3gsqOGMjQrPdbhiMQdJRORFrg7v31jFYZxs6ZMEWmUkolIC2bkFfDyok3cPGUMg/qkxTockbikZCLSjBWbd/Pzl5ZxwpgcTeQo0gwlE5EmlFXVctPf59M7LYUHrphIUjc96S7SlCafMzGz3/PlWXu/xN1viUpEInHi5y8uZX1JOf+4brKedBdpQXM1kzxgHtADmASsCV4Tge7RD00kdt5ctoXnFxTxvSljmTwqO9bhiMS9Jmsm7v44gJl9BzjB3WuD7f8L/LNjwhPpeKXl1dzxwlLGDezFzVPUTyISiUj6TPoCvRpsZwZlIgnply8vY2dFNb+79DAteCUSoUjm5voNsMDM3gUMOAm4K5pBicTKG0u38OLCTXz/9LGMH9Q71uGIdBrNJhMz6wasIrzm+jFB8W3BlPAiCWNp0S7++F4+ry/dwriBvTQMWGQ/NZtM3D1kZg+7+xHASx0Uk0iHWVSwkwdmr+bdVcX0TE3mu6eM5tsnjFLzlsh+iqSZ620zuxh4PpgGXqTTW1q0i/vfWs3bK7fRJz2Fn5x1EP9x7HB69UiJdWginVIkyeQG4IdArZlVEu43cXfv1fxpIvHns+Iy/vfNVby2ZAu901L48ZkHcs1xI+ipJCLSJi0mE3fv2RGBiETTnsoafv3aSmbkFZCa3I1bpozh2yeNUk1EpJ1EtNJisLb6WMIPMALg7h9EKyiR9jR/Yym3Pr2AotK9XH3sCG6eMkZPtIu0sxaTiZl9G7gVGAIsBCYDHwNTohuaSNuUV9Xy6IfrePDtNQzo1YMZNxxL7oisWIclkpAiqZncChwFzHH3U83sYODX0Q1LpPU2llTwxMfrmZ5XwJ7KWs47bCD3XDiB3mlq0hKJlkiSSaW7V5oZZpbq7ivN7KBoBmVm3wNuAuqAV939p0H5z4Brg/Jb3H1WUH428CCQBDzi7r+JZnwSn5YW7eJP73/G60s2082McyYM5BvHDefI4aqNiERbJMmk0Mz6AC8Cb5lZKbAhWgGZ2anAVOBwd68ys/5B+TjgCmA8MAiYbWYHBqc9DJwBFAJzzWymuy+PVowSX/K3lXH3K8t5f3UxmanJXH/SaL5x3AgG9O7R8ski0i4iGc11YfD2rmBKld7AG1GM6TvAb9y9Krj+tqB8KvB0UL7OzPKBo4N9+e6+FsDMng6OVTJJcLV1IR75cB33vbWatJQkfnLWQVw1ebias0RiIJIO+LuBD4B/ufv70Q+JA4ETzeweoBL4sbvPBQYDcxocVxiUARTsU34MjTCz64HrAYYNG9bOYUtHWliwk1/MXMaigp2cOe4AfnXhofTvqZqISKxE0sy1Fvga8JCZ7SE8/fwH7t7q6VXMbDYwoJFddwYxZREeNXYUMMPMRrX2Wg25+zRgGkBubq6e5u+E1mzdw/+8uYpZy7aSndGdh752BOcfNhAzrYIoEkuRNHP9BfiLmQ0ALgN+TPiv+1Y/zOjupze1L1g/pX7qlk/NLATkAEXA0AaHDgnKaKZcEoC781F+CX+bs563lm8lvXsyPzj9QK49cSSZqRE9KiUiURZJM9cjwDhgK+FaySXA/CjG9CJwKvBu0MHeHdgOzAT+YWb3Ee6AHwt8Snh6l7FmNpJwErkC+HoU45MO4u7MXLSJB99ew9ricrIyunPDyaO57sRRZGVosU+ReBLJn3XZhIfc7gR2ANvrV12MkseAx8xsKVANXBPUUpaZ2QzCHeu1wE3uXgdgZjcDs4I4H3P3ZVGMTzrAmq17+K+XljJn7Q7GD+rF/ZcfzjmHDqRHSlKsQxORRlikEwGb2SHAWcAPgCR3HxLNwKItNzfX8/LyYh2G7GNXRQ2/f2cNf/3XejJSk/np2QdxxVHDSOqmPhGReGBm89w9d9/ySJq5zgNOJLzCYh/gHbQGvLSzmroQf5+zgQfeXsOuvTVcnjuUn5x1ENmaQ0ukU4ikmetswsnjQXffFOV4pAuqrKnj2sfn8lF+CcePyebOc8cxbpBWOBDpTCIZzXWzmQ0n3Am/yczSgGR33xP16CTh1daFuPXpBXyUX8JvLprA5UcN1TBfkU6oxbVJzew64Fngz0HREMIjrkTaxN2584WlzFq2lZ+fN44rjh6mRCLSSUWy0PVNwPHAbgB3XwP0j2ZQkvjcnXteXcH0vAJumTKGb50wMtYhiUgbRNJnUuXu1fV/MZpZMqCnx6XVQiHnFzOX8bc5G/jGcSP4wRkHtnySiMS1SJLJ+2Z2B5BmZmcA3wVejm5Ykqhq60Lc9twSnptfyA0njeL2cw5W05ZIAoikmet2oBhYAtwAvObud0Y1KklItXUhvj99Ic/NL+QHpx+oRCKSQFpMJu4ecvf/5+6XuvslwAYze6sDYpMEUhdyfvTMIl5ZvJmfnXMwt54+VolEJIE0mUzMbIqZrTazMjN70swmmFke8H+AP3VciNLZhULOT59dzEsLN/HTsw/ihpNHxzokEWlnzdVM/pfw7MDZhIcGfwz81d2PdPfnOyI46fzcnTtfXMpz8wv54RkH8t1TxsQ6JBGJguY64N3d3wvev2hmRe7+hw6ISRLI/bPX8NSnG7np1NHcctrYWIcjIlHSXDLpY2YXNTy24bZqJ9KSf3yykYfeXsNluUP48ZkHxTocEYmi5pLJ+8D5DbY/aLDtgJKJNGn28q3854tLOOWgftxz4QR1toskuCaTibt/syMDkcSxtGgX33tqAeMH9ebhr08iJSmSEegi0pnp/3JpV9v2VHL9E3n0SU/h0W/kkqFldUW6BP2fLu2msqaOG/42jx0V1Tx743H079kj1iGJSAdRMpF2UT8D8IKNO/njlZM4dHDvWIckIh0okinobzKzPg22+5rZd6MblnQ2T36ykefmF3LraWM5d8LAWIcjIh0skj6T69x9Z/2Gu5cC10UvJOlsFhXs5O6Xl3PKQf24Vc+SiHRJkSSTJGswrtPMkoDu0QrIzCaa2RwzW2hmeWZ2dFBuZvaQmeWb2WIzm9TgnGvMbE3wuiZascm/Ky2v5rt/n0+/nqncf9lEunXTEGCRriiSPpM3gOlmVr/S4g1BWbTcC/zS3V83s3OD7VOAc4CxwesYwvODHWNmWcAvgFzCz7/MM7OZQQ1KoigUcn44YyHb9lTy7I3H0Tcjan9jiEiciySZ3EY4gXwn2H4LeCRqEYUTQq/gfW9gU/B+KvCEuzswx8z6mNlAwonmLXffARDMaHw28FQUYxTgt7NW8u6qYu6eOp7Dh/Zp+QQRSVgtJhN3DxGuBXTUTMHfB2aZ2f8QboY7LigfDBQ0OK4wKGuq/N+Y2fWEJ69k2LBh7Rt1FzMjr4A/v7+WK48ZxlWTh8c6HBGJsSaTiZnNcPfLzGwJjSzT6+6HtfaiZjYbGNDIrjuB04AfuPtzZnYZ8Chwemuv1ZC7TwOmAeTm5mrp4Vaas7aEO19Ywgljcrjrq+M1VYqINFszuTX4eV57X9Tdm0wOZvZEg2s/wxdNakXA0AaHDgnKigg3dTUsf6+dQpV9FO+p4jtPzmNoVjoPX6mpUkQkrMlvAnffHPzc0NgrijFtAk4O3k8B1gTvZwJXB6O6JgO7ghhnAWcGz7/0Bc4MyiQKHpi9mj2VtUz7j1x6p6XEOhwRiRPNNXPtoZHmrXru3qupfW10HfCgmSUDlQR9HMBrwLlAPlABfDOIY4eZ3Q3MDY777/rOeGlf+dvKeHpuAf8xeThj+mfGOhwRiSPNzRrcEyD4ot4M/A0w4Eogao84u/uHwJGNlDtwUxPnPAY8Fq2YJOw3r68kPSWJ703Raoki8mWRNHh/1d3/6O573H23u/+J8DBd6UI+WVvC7BVbufGU0WRnpsY6HBGJM5Ekk3Izu9LMksysm5ldCZRHOzCJH+7Or19fyYBePfjW8SNjHY6IxKFIksnXgcuArcA24NKgTLqIp+cWsKhgJz8680DSuifFOhwRiUORPLS4HjVrdVkbSsq5+5XlHD8mm4snDYl1OCISpyKZgn6Imb1gZtuC13Nmpm+VLqAu5PxoxiKSuhm/u+RwTeIoIk2KpJnrL4Sf8RgUvF4OyiTBTftgLXkbSvnvqeMZ1Cct1uGISByLJJn0c/e/uHtt8Por0C/KcUmMrd66h/veWsW5EwZwwcRGpzoTEflcJMmkxMyuCkZzJZnZVUBJtAOT2Lr3jZX0SEniVxdM0NxbItKiSJLJtwiP5tpC+OHFSwiePpfElLd+B7NXbOPGk0eTpTVKRCQCkYzm2gB8tQNikTjg7tz7xir69Uzlm8ePiHU4ItJJNDc310/d/V4z+z2NT0F/S1Qjk5h4b3Uxn67fwd0XHEp690jWThMRab5msiL4mdcRgUjshULhWsnw7HSuOGpoyyeIiASam+jx5eDn4/VlZtYNyHT33R0Qm3SwZ+cXsmLzbh68YqLWKRGR/RLJQ4v/MLNeZpYBLAWWm9lPoh+adKStuyv51SvLOWpEX84/bFCswxGRTiaSPz/HBTWRC4DXgZHAf0Q1KulQ7s6dLyylqjbEvXrSXURaIZJkkmJmKYSTyUx3r6GZRbOk85m5aBOzV2zlx2cexMicjFiHIyKdUCTJ5M/AeiAD+MDMhgPqM0kQxXuq+MXMZUwc2odvnaDp5UWkdSJ5zuQh4KEGRRvM7NTohSQd6b63VlFeVcvvLjmMJDVviUgrRdIBn21mD5nZfDObZ2YPAr07IDaJsvXby5mRV8jXjx7G2AN6xjocEenEImnmehooBi4mPJVKMTC9LRc1s0vNbJmZhcwsd599PzOzfDNbZWZnNSg/OyjLN7PbG5SPNLNPgvLpZqb5PyL0wOzVpCQZN2lNdxFpo0iSyUB3v9vd1wWvXwEHtPG6S4GLgA8aFprZOOAKYDxwNvDH+gkmgYeBc4BxwNeCYwF+C9zv7mOAUuDaNsbWJazasoeXFm3imuNG0L9nj1iHIyKdXCTJ5E0zuyJY/72bmV0GzGrLRd19hbuvamTXVOBpd69y93VAPnB08Mp397XuXk24tjTVwtPZTgGeDc5/nPCoM2nBfW+tIrN7MjeeNDrWoYhIAogkmVwH/AOoCl5PAzeY2R4za+9RXYOBggbbhUFZU+XZwE53r92nvFFmdr2Z5ZlZXnFxcbsG3pksLtzJrGVbufbEkfTVrMAi0g4iGc3Vqp5ZM5sNDGhk153u/lJrfmdbufs0YBpAbm5ul31W5q//Wk/P1GSu1VBgEWknzc0afJW7Pxm8P97dP2qw72Z3/0Nzv9jdT29FPEVAwxkGhwRlNFFeAvQxs+SgdtLweGlEZU0dby7byjmHDqBnj5RYhyMiCaK5Zq4fNnj/+332fSsKsUB4rfkrzCzVzEYCY4FPgbnA2GDkVnfCnfQz3d2BdwmPMgO4BohJraezeH91MWVVtZx/uObfEpH201wysSbeN7a9X8zsQjMrBI4FXjWzWQDuvgyYASwH3gBucve6oNZxM+GO/xXAjOBYgNuAH5pZPuE+lEfbEluie3nRJrIyunPc6OxYhyIiCaS5PhNv4n1j2/vF3V8AXmhi3z3APY2Uvwa81kj5WsKjvaQFFdW1vL1iGxcfOZhkTTEvIu2ouWRysJktJlwLGR28J9geFfXIpN3NXrGNvTV1nKcp5kWknTWXTA7psCikQ7yyaBMH9ErlqBFZsQ5FRBJMcystbujIQCS6dlfW8N6qYq6aPFwTOopIu1PDeRfx5rKtVNeFOP/wgbEORUQSkJJJF/HCgkKGZqUxcWifWIciIglIyaQL2FhSwUf5JVx65FDC05mJiLSvViUTM7urneOQKHpmXgHdDC45ckisQxGRBNXamsm8do1CoqYu5DyTV8hJB/ZjUJ+0WIcjIgmqVcnE3V9u70AkOj5YXcyW3ZVccdTQlg8WEWmlFmcNNrOHGineBeTFavZfidzTczeSndGdKQe3dT0zEZGmRVIz6QFMBNYEr8MIz857rZk9EMXYpI2K91QF06cMoXuyxlqISPS0WDMhnDyOd/c6ADP7E/BP4ARgSRRjkzZ6dl4htSHnslw1cYlIdEXy52pfILPBdgaQFSSXqqhEJW22pHAXD769mhPH5jCmf2bLJ4iItEEkNZN7gYVm9h7hSR5PAn5tZhnA7CjGJq20dXcl335iLtkZqdx/+cRYhyMiXUAky/Y+amav8cU073e4+6bg/U+iFpm0SmVNHdc/kceeylqe+85x5GSmxjokEekCIhnN9TLwD8IrG5ZHPyRpi7tmLmNx0S6m/UcuhwzsFetwRKSLiKTP5H+AE4HlZvasmV1iZj2iHJe0QsGOCp6ZV8g3jhvBGeM0FFhEOk4kzVzvA++bWRIwBbgOeAzQn71x5tEP12HA9Sdp7TIR6ViRdMBjZmnA+cDlwCTg8WgGJfuvtLya6XML+OrEQQzsrWlTRKRjRdJnMoNw5/sbwB+A9909FO3AZP88OWcDe2vqVCsRkZiIpM/kUWC0u9/o7u8Cx5nZw225qJldambLzCxkZrkNys8ws3lmtiT4OaXBviOD8nwze8iCudTNLMvM3jKzNcHPvm2JrTOqrKnj8Y/Xc8pB/Th4gFofRaTjtZhM3H0WcJiZ3Wtm64G7gZVtvO5S4CLgg33KtwPnu/sE4Brgbw32/Ylwf83Y4HV2UH478La7jwXeDra7lOfmF7K9rFq1EhGJmSabuczsQOBrwWs7MB0wdz+1rRd19xXBNfYtX9BgcxmQZmapQBbQy93nBOc9AVwAvA5MBU4JznkceA+4ra0xdhYV1bX88d3PmDC4N8eOyo51OCLSRTVXM1lJePTWee5+grv/HqjrmLAAuBiY7+5VwGCgsMG+wqAM4AB33xy83wI0OSbWzK43szwzyysuLo5GzB3ugdlrKNq5l//8yiFaRVFEYqa5ZHIRsBl418z+n5mdRng6lYiY2WwzW9rIa2JyAbUAAA8iSURBVGoE544HfgvcEOn1ANzdAW9m/zR3z3X33H79+u3Pr45LS4t28cg/1/K1o4dyjGolIhJDTTZzufuLwIvBHFxTge8D/YNZg19w9zeb+8XufnprAjKzIcALwNXu/llQXER42vt6Q4IygK1mNtDdN5vZQGBba67b2dTWhbj9+cVkZaRy+9mHxDocEeniIumAL3f3f7j7+YS/xBcQpT4JM+sDvArc7u4fNYhhM7DbzCYHo7iuBuoX5ppJuLOe4GeXWLDrr/9az9Ki3dz11XH0Tk+JdTgi0sXt14pJ7l4aNBWd1paLmtmFZlYIHAu8amazgl03A2OAn5vZwuDVP9j3XeARIB/4jHDnO8BvgDPMbA1werCd0JZv2s3vZq3itIP785UJA2MdjogIFu5m6Hpyc3M9Ly8v1mHst92VNXz19x+yt6aOV285UbMCi0iHMrN57p67b3lE06lIfHB3fvLMIgpK9zL9+slKJCISN7QweCfy6IfrmLVsKz8752ByR2TFOhwRkc8pmXQSq7fu4bdvrOSs8Qdw7QkjYx2OiMiXKJl0AqGQc8fzS8hMTebXF07Qw4kiEneUTDqBp+cWkLehlDvOPYRs9ZOISBxSMolz2/ZU8n9eX8HkUVlccuSQlk8QEYkBJZM4d/crK6iqCXGPmrdEJI4pmcSxNVv38PKiTdx48ihG98uMdTgiIk1SMolj0+cWkNzNuPq4EbEORUSkWUomcaq6NsTzC4o4Y9wBejhRROKekkmcmr1iKzvKq7nsqKGxDkVEpEVKJnHq6bkFDOzdg5PGdv51V0Qk8SmZxKGinXv555piLs0dSlI3jeASkfinZBKHnskrAOBSPVciIp2EkkmcqQs5z+QVcsKYHIZmpcc6HBGRiCiZxJm/f7KBop17ufKY4bEORUQkYkomcWTLrkrufWMVJ47N4azxB8Q6HBGRiCmZxJFfzFxKTV2IX11wqKZOEZFORckkTry5bAuzlm3l1tPHMjw7I9bhiIjsFy3bG2Puztz1pfz8pWUcPKAn1504KtYhiYjst5jUTMzsUjNbZmYhM/u3henNbJiZlZnZjxuUnW1mq8ws38xub1A+0sw+Ccqnm1n3jvocbeHuvLxoE1Mf/ojL/vwxVbV13HvJYaQkqbIoIp1PrL65lgIXAR80sf8+4PX6DTNLAh4GzgHGAV8zs3HB7t8C97v7GKAUuDZaQe+rrKqW7WVVrTr3reVb+d5TCyirquVXFxzKv24/jcOG9GnnCEVEOkZMkom7r3D3VY3tM7MLgHXAsgbFRwP57r7W3auBp4GpFu6lngI8Gxz3OHBB9CL/snteXc4V0+a06txn5hXSr2cqb37/JK6aPJy07kntHJ2ISMeJqzYVM8sEbgN+uc+uwUBBg+3CoCwb2OnutfuUN/X7rzezPDPLKy4ubnO8iwt3kb+tjIrq2pYPbqCkrIp3V27jwiMGk6xmLRFJAFH7JjOz2Wa2tJHX1GZOu4twk1VZNGJy92nunuvuuf36tW0CxVDI+aw4HOba4vL9OvflRZuoDTkXT9J0KSKSGKI2msvdT2/FaccAl5jZvUAfIGRmlcA8oOFc7EOAIqAE6GNmyUHtpL486op27qWyJgTAZ8VlHDq4d8TnPr+giPGDenHQgJ7RCk9EpEPF1dBgdz+x/r2Z3QWUufsfzCwZGGtmIwkniyuAr7u7m9m7wCWE+1GuAV7qiFjzi7+oPH22LfKK1Jqte1hcuIv/Om9cyweLiHQSsRoafKGZFQLHAq+a2azmjg9qHTcDs4AVwAx3r++gvw34oZnlE+5DeTR6kX+hPoFkZXT/UmJpyXPzi0jqZkydOChaoYmIdLiY1Ezc/QXghRaOuWuf7deA1xo5bi3h0V4dKn9bGVkZ3Zk0rA/5EdZM6kLOiwuKOOXAflqKV0QSioYStVL+tjLG9MtkdP9M1m+voLYu1OI5763axpbdlVykjncRSTBKJq3g7uQXlzG6fyaj+2VSXReioHRvs+dU1tTx368sZ2ROBqeP699BkYqIdAwlk1YoKa9mZ0UNY/pnMqZ/JtByJ/zv31nDhpIK7rngUFKT9YCiiCQWJZNWqO8jGRvUTIBmO+FXb93Dn99fy8WThnDcmJwOiVFEpCPF1dDgzqI+mYzpn0nvtBT69Uz9Us2kti7E+pJyeqQk0TM1hTueX0LPHsnc+ZVDYhWyiEhUKZm0Qv62MjK6JzGwdw8AxvTL/FLN5KG31/DQO/lfOud3lxxGVkanmNBYRGS/KZm0wmdB53v9aoij+2cwc+Em3J3quhB//2Qjx4zM4qJJg9lTWUt2ZncumNjklGEiIp2ekkkr5G8r49hR2Z9vj+mXye7KWorLqvgofzsl5dU8MGUMJ45t2/xfIiKdhTrg91NZVS2bd1UyOhjFBXz+/rNt5Tzx8QZG5WRw/Gh1tItI16Fksp8+a9D5Xq/+/UsLi1iwcSdXTR5Ot24Wk/hERGJByWQ/5TeSTAb06kFG9ySm5xWQlpLExUfqCXcR6VqUTPZTfnEZKUnG8Kz0z8vMjNH9M3GHC44YTO+0lBhGKCLS8ZRM9lP+tjJGZGf82wqJY4KHF68+dngswhIRiSmN5tpPlx45hPJGlum96tjhHDSgJ4cM7BWDqEREYkvJZD+dOX5Ao+WThvVl0rC+HRyNiEh8UDOXiIi0mZKJiIi0mZKJiIi0mZKJiIi0WUySiZldambLzCxkZrn77DvMzD4O9i8xsx5B+ZHBdr6ZPWTBLItmlmVmb5nZmuCnesFFRDpYrGomS4GLgA8aFppZMvAkcKO7jwdOAWqC3X8CrgPGBq+zg/LbgbfdfSzwdrAtIiIdKCbJxN1XuPuqRnadCSx290XBcSXuXmdmA4Fe7j7H3R14ArggOGcq8Hjw/vEG5SIi0kHirc/kQMDNbJaZzTeznwblg4HCBscVBmUAB7j75uD9FuCAjglVRETqRe2hRTObDTT2hN+d7v5SM/GcABwFVABvm9k8YFck13R3NzNvJqbrgeuDzTIza6x2FIkcYHsrz+3MuuLn7oqfGbrm59Znjkyjc0ZFLZm4++mtOK0Q+MDdtwOY2WvAJML9KA2n4h0CFAXvt5rZQHffHDSHbWsmpmnAtFbE9SVmlufuuS0fmVi64ufuip8Zuubn1mdum3hr5poFTDCz9KAz/mRgedCMtdvMJgejuK4G6ms3M4FrgvfXNCgXEZEOEquhwReaWSFwLPCqmc0CcPdS4D5gLrAQmO/urwanfRd4BMgHPgNeD8p/A5xhZmuA04NtERHpQDGZ6NHdXwBeaGLfk4SbtfYtzwMObaS8BDitvWNsQZubyjqprvi5u+Jnhq75ufWZ28DCI21FRERaL976TEREpBNSMhERkTZTMtlPZna2ma0K5ghLyKlbzGyomb1rZsuDOdJuDcoTfh40M0syswVm9kqwPdLMPgnu93Qz6x7rGNubmfUxs2fNbKWZrTCzYxP9XpvZD4J/20vN7Ckz65GI99rMHjOzbWa2tEFZo/fWwh4KPv9iM5u0P9dSMtkPZpYEPAycA4wDvmZm42IbVVTUAj9y93HAZOCm4HN2hXnQbgVWNNj+LXC/u48BSoFrYxJVdD0IvOHuBwOHE/78CXuvzWwwcAuQ6+6HAknAFSTmvf4rX8xjWK+pe3sOX8x9eD3h+RAjpmSyf44G8t19rbtXA08Tnhssobj7ZnefH7zfQ/jLZTAJPg+amQ0BvkJ4CDrBM01TgGeDQxLxM/cGTgIeBXD3anffSYLfa8IjWdOC59nSgc0k4L129w+AHfsUN3VvpwJPeNgcoE/wIHhElEz2z2CgoMF2wznCEpKZjQCOAD4h8edBewD4KRAKtrOBne5eG2wn4v0eCRQDfwma9x4xswwS+F67exHwP8BGwklkFzCPxL/X9Zq6t236flMykSaZWSbwHPB9d9/dcF8we3PCjCs3s/OAbe4+L9axdLBkwlMW/cndjwDK2adJKwHvdV/Cf4WPBAYBGfx7U1CX0J73Vslk/xQBQxtsN5wjLKGYWQrhRPJ3d38+KN5aX+1taR60Tuh44Ktmtp5w8+UUwn0JfYKmEEjM+10IFLr7J8H2s4STSyLf69OBde5e7O41wPOE73+i3+t6Td3bNn2/KZnsn7nA2GDUR3fCnXYzYxxTuwv6Ch4FVrj7fQ12Jew8aO7+M3cf4u4jCN/Xd9z9SuBd4JLgsIT6zADuvgUoMLODgqLTgOUk8L0m3Lw1OZgD0PjiMyf0vW6gqXs7E7g6GNU1GdjVoDmsRXoCfj+Z2bmE29aTgMfc/Z4Yh9TuzOwE4J/AEr7oP7iDcL/JDGAYsAG4zN337dzr9MzsFODH7n6emY0iXFPJAhYAV7l7VSzja29mNpHwoIPuwFrgm4T/0EzYe21mvwQuJzxycQHwbcL9Awl1r83sKcIr1uYAW4FfAC/SyL0NEusfCDf5VQDfDKaxiuxaSiYiItJWauYSEZE2UzIREZE2UzIREZE2UzIREZE2UzIREZE2UzIRaSdmVmdmCxu8mp0c0cxuNLOr2+G6680sp62/R6QtNDRYpJ2YWZm7Z8bguusJz4C7vaOvLVJPNRORKAtqDvea2RIz+9TMxgTld5nZj4P3twTrxyw2s6eDsiwzezEom2NmhwXl2Wb2ZrAexyOANbjWVcE1FprZn4NlE0SiTslEpP2k7dPMdXmDfbvcfQLhJ4wfaOTc24Ej3P0w4Mag7JfAgqDsDuCJoPwXwIfuPh54gfCTzJjZIYSf6j7e3ScCdcCV7fsRRRqX3PIhIhKhvcGXeGOeavDz/kb2Lwb+bmYvEp7uAuAE4GIAd38nqJH0Irz+yEVB+atmVhocfxpwJDA3PDMGaSTWBI0Sx5RMRDqGN/G+3lcIJ4nzgTvNbEIrrmHA4+7+s1acK9ImauYS6RiXN/j5ccMdZtYNGOru7wK3Ab2BTMKTbV4ZHHMKsD1YV+YD4OtB+TlA/frsbwOXmFn/YF+WmQ2P4mcS+ZxqJiLtJ83MFjbYfsPd64cH9zWzxUAV8LV9zksCngyW0DXgIXffaWZ3AY8F51XwxbThvwSeMrNlwL8IT6mOuy83s/8E3gwSVA1wE+GZYUWiSkODRaJMQ3elK1Azl4iItJlqJiIi0maqmYiISJspmYiISJspmYiISJspmYiISJspmYiISJv9f2EKDzjB69llAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"47waMA13q3ro"},"source":["If training proceeds correctly, the average episodic reward will increase with time.\n","\n","Feel free to try different learning rates, `tau` values, and architectures for the\n","Actor and Critic networks.\n","\n","The Inverted Pendulum problem has low complexity, but DDPG work great on many other\n","problems.\n","\n","Another great environment to try this on is `LunarLandingContinuous-v2`, but it will take\n","more episodes to obtain good results."]},{"cell_type":"code","metadata":{"id":"uxI0C1Nuq3ro"},"source":["# Save the weights\n","actor_model.save_weights(\"pendulum_actor.h5\")\n","critic_model.save_weights(\"pendulum_critic.h5\")\n","\n","target_actor.save_weights(\"pendulum_target_actor.h5\")\n","target_critic.save_weights(\"pendulum_target_critic.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tbf5P_gxq3rp"},"source":["Before Training:\n","\n","![before_img](https://i.imgur.com/ox6b9rC.gif)"]},{"cell_type":"markdown","metadata":{"id":"KJtiDAFYq3rp"},"source":["After 100 episodes:\n","\n","![after_img](https://i.imgur.com/eEH8Cz6.gif)"]}]}